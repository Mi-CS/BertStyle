{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(\"../src\")\n",
    "sys.path.append(\"../methods\")\n",
    "\n",
    "# Basic imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Deep Learning\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Tokenize sentences\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Utils\n",
    "from dataset_building import build_dataset\n",
    "from model import init_model\n",
    "from trainer import train_epoch\n",
    "\n",
    "# Measurements\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from classification import LogClassification, train_classifier\n",
    "from clustering import KMeansAuthors\n",
    "\n",
    "# Progress bar\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading\n",
    "\n",
    "Here, as an example, we are taking a 3 author subset from the Reuters dataset just to simply show how the code works. For an actual training, proper training and test sets must be defined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_json(\"../data/reuters_sample.json\")\n",
    "\n",
    "# Clean and tokenize\n",
    "df.text = df.text.progress_apply(lambda x: x.lower())\n",
    "df.text = df.text.progress_apply(sent_tokenize)\n",
    "\n",
    "# Build dataset\n",
    "dataset = build_dataset(df.text,\n",
    "                masking_percentage=0.5,\n",
    "                max_pairs_per_doc = 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = init_model(device)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    \n",
    "    # Construct DataLoader\n",
    "    dataloader = DataLoader(dataset,\n",
    "                            batch_size = 16, \n",
    "                            shuffle = True)\n",
    "\n",
    "    train_epoch(model = model,\n",
    "                tokenizer = tokenizer,\n",
    "                dataloader = dataloader,\n",
    "                optimizer = optimizer,\n",
    "                criterion = criterion,\n",
    "                device = device,\n",
    "                print_each = 500, \n",
    "                disable_progress_bar = False)\n",
    "\n",
    "    # Create folder if it doesn't exist\n",
    "    if not os.path.isdir(\"saved_models\"):\n",
    "        os.mkdir(\"saved_models\")\n",
    "\n",
    "    # Save model weights after epoch\n",
    "    save_path = f\"saved_models/model_{epoch}epoch.pt\"\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(\"Model saved.\\n\\n\")\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style representations\n",
    "\n",
    "With the trained models, it is then easy to obtain the style representations for an input text. One simply needs to load the model, set it to the evaluation mode, and perform the forward pass for the given text(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = init_model(device)\n",
    "\n",
    "# Load trained model weights\n",
    "state = torch.load(f\"saved_models/saved_model.pt\", map_location=torch.device('cpu'))\n",
    "\n",
    "# This bit corrects the layer names in the saved PyTorch weights, so it can \n",
    "# match them. \n",
    "state_corrected = {key.replace(\"module.\", \"\"):value for key, value in state.items()}\n",
    "model.load_state_dict(state_corrected)\n",
    "\n",
    "# Set model to evaluation\n",
    "_ = model.to(device)\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_style_representations(sentence, tokenizer, model, device):\n",
    "    \"\"\"Simple method to obtain the style representation of a sentence\"\"\"\n",
    "\n",
    "    # Tokenize sentence\n",
    "    toks = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    tok_ids = toks.input_ids[:, :512]\n",
    "    att_mask = toks.attention_mask[:, :512]\n",
    "    tok_ids = tok_ids.to(device)\n",
    "    att_mask = att_mask.to(device)\n",
    "\n",
    "    # Forward pass, keeping only [CLS] from the last hidden state\n",
    "    out = model(tok_ids, att_mask, return_lhs=True)\n",
    "    return out.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since for training we are using three sentences, we will\n",
    "# also use 3 sentences as input for evaluating our model\n",
    "def chunk_text(sent_list):\n",
    "    \"\"\"Group sentences into chunks of 3 sentences\"\"\"\n",
    "    total_length = len(sent_list)\n",
    "    chunks =  [sent_list[i:i+3] for i in \n",
    "                    range(0, total_length, 3)]\n",
    "\n",
    "    # Remove last chunk if it is too small\n",
    "    if len(chunks[-1]) != 3: \n",
    "        del chunks[-1]\n",
    "    \n",
    "    chunks = [\" \".join(chunk) for chunk in chunks]\n",
    "    return chunks\n",
    "\n",
    "df.text = df.text.apply(chunk_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build new column containing the style representations\n",
    "df[\"style_representations\"] = df.text.progress_apply(lambda sentences: \n",
    "                                np.vstack([get_style_representations(sent, tokenizer, model, device)\n",
    "                                                   for sent in sentences]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation methods\n",
    "\n",
    "## Method 1: Dimension reduction + K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize data and apply PCA\n",
    "data = StandardScaler().fit_transform(np.vstack(df.style_representations))\n",
    "X = PCA(n_components=5).fit_transform(data)\n",
    "\n",
    "# Instantiate method\n",
    "cl = KMeansAuthors(n_authors=3)\n",
    "\n",
    "# Create an author label for each point\n",
    "auth_labels = [[author]*n_chunks for author, n_chunks in \n",
    "                zip(df.author, df.style_representations.apply(len))]\n",
    "\n",
    "# Flatten\n",
    "auth_labels = [x for y in auth_labels for x in y]\n",
    "\n",
    "# Fit data. Pass author labels to assign one author per cluster\n",
    "cl.fit(X, auth_labels)       \n",
    "\n",
    "# Metrics\n",
    "author_pred = cl.predict_document(X, df.style_representations.apply(len).to_numpy())\n",
    "print(classification_report(y_true = df.author, y_pred = author_pred, zero_division=0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate classifier\n",
    "classifier = LogClassification(n_feat = len(df.style_representations[0][0]),\n",
    "                               n_class = len(df.author.unique()))\n",
    "\n",
    "# Create labels for authors\n",
    "auth_dict = dict(zip(set(df.author), range(len(df.author.unique()))))\n",
    "labels_train = [[auth_dict[auth]]*n_chunks for auth, n_chunks in zip(df.author,\n",
    " df.style_representations.apply(len))]\n",
    "\n",
    "# Flat labels and chunks \n",
    "labels_train = [x for y in labels_train for x in y]\n",
    "chunks = [x for chunk in df.style_representations for x in chunk]\n",
    "\n",
    "# Build dataset as tuples (chunk, label)\n",
    "dataset = list(zip(chunks, labels_train))\n",
    "\n",
    "# Just for the purpose of showing the code, we use \n",
    "# the dataframe we already have as also test set.\n",
    "df_test = df.copy()\n",
    "\n",
    "labels_test = [[auth_dict[auth]]*n_chunks for auth, n_chunks in zip(df_test.author,\n",
    " df_test.style_representations.apply(len))]\n",
    "\n",
    "# Flat labels and chunks \n",
    "labels_test = [x for y in labels_test for x in y]\n",
    "chunks = [x for chunk in df_test.style_representations for x in chunk]\n",
    "\n",
    "# Build dataset as tuples (chunk, label)\n",
    "dataset_test = list(zip(chunks, labels_test))\n",
    "\n",
    "# Normalize (if necessary) and build dataset\n",
    "scaler_train = StandardScaler().fit([x[0] for x in dataset])\n",
    "dataset = [(scaler_train.transform([x[0]])[0], x[1]) for x in dataset]\n",
    "dataset_test = [(scaler_train.transform([x[0]])[0], x[1]) for x in dataset_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train  classifier and generate json files with results\n",
    "train_classifier(classifier, \n",
    "                 dataset, \n",
    "                 dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check measurements\n",
    "pd.read_json(\"training_measurements.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "torch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "acd62c5062357039a923d5a2091962054c169ae848350ae1587a2a7c600dc804"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
